{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T16:49:55.066921Z",
     "start_time": "2025-04-06T16:35:45.009509Z"
    }
   },
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ========= å‚æ•°é…ç½® ========= #\n",
    "API_KEY = 'AIzaSyAPFSYKG9lnXD2pBYqLd0RUweQwMpoQfVo'  # æ›¿æ¢ä¸ºä½ çš„ API Key\n",
    "SEARCH_QUERY = 'artificial intelligence'  # æœç´¢å…³é”®è¯\n",
    "MAX_RESULTS = 500  # æœ€å¤šè·å–è§†é¢‘æ•°é‡\n",
    "API_UNITS_LIMIT = 10000       # YouTube API æ¯æ—¥é…é¢\n",
    "SAFETY_THRESHOLD = 9000       # å®‰å…¨é˜ˆå€¼ï¼ˆåˆ°è¾¾å³è‡ªåŠ¨åœæ­¢ï¼‰\n",
    "SAVE_PATH = 'D:/COMM5007/Data/'\n",
    "\n",
    "# ========= åˆå§‹åŒ– ========= #\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "video_data = []\n",
    "comment_data = []\n",
    "estimated_units_used = 0  # é…é¢ä¼°ç®—è®¡æ•°å™¨\n",
    "\n",
    "# ========= æœç´¢è§†é¢‘ ========= #\n",
    "def search_videos(query, max_results=500):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    total_fetched = 0\n",
    "\n",
    "    while total_fetched < max_results:\n",
    "        request = youtube.search().list(\n",
    "            q=query,\n",
    "            part='id',\n",
    "            type='video',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            videos.append(item['id']['videoId'])\n",
    "        total_fetched += len(response['items'])\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    return videos\n",
    "\n",
    "# ========= è·å–è§†é¢‘è¯¦æƒ… ========= #\n",
    "def get_video_details(video_ids):\n",
    "    details = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part='snippet,statistics',\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            details.append({\n",
    "                'video_id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'description': item['snippet']['description'],\n",
    "                'publishedAt': item['snippet']['publishedAt'],\n",
    "                'channelTitle': item['snippet']['channelTitle'],\n",
    "                'tags': ','.join(item['snippet'].get('tags', [])),\n",
    "                'categoryId': item['snippet']['categoryId'],\n",
    "                'viewCount': int(item['statistics'].get('viewCount', 0)),\n",
    "                'likeCount': int(item['statistics'].get('likeCount', 0)),\n",
    "                'commentCount': int(item['statistics'].get('commentCount', 0)),\n",
    "            })\n",
    "        time.sleep(1)\n",
    "    return details\n",
    "\n",
    "# ========= è·å–æ‰€æœ‰è¯„è®º ========= #\n",
    "def get_comments(video_id, max_comments=None):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat='plainText'\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comments.append({\n",
    "                    'video_id': video_id,\n",
    "                    'author': top_comment['authorDisplayName'],\n",
    "                    'comment': top_comment['textDisplay'],\n",
    "                    'likeCount': top_comment['likeCount'],\n",
    "                    'publishedAt': top_comment['publishedAt']\n",
    "                })\n",
    "\n",
    "                if max_comments and len(comments) >= max_comments:\n",
    "                    return comments\n",
    "\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "# ========= ä¿å­˜æ•°æ®å‡½æ•° ========= #\n",
    "def save_data(video_data, comment_data, video_file='videos.csv', comment_file='comments_partial.csv'):\n",
    "    # è‡ªåŠ¨åˆ›å»ºè·¯å¾„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "    # æ‹¼æ¥å®Œæ•´è·¯å¾„\n",
    "    video_path = os.path.join(SAVE_PATH, video_file)\n",
    "    comment_path = os.path.join(SAVE_PATH, comment_file)\n",
    "\n",
    "    # ä¿å­˜ CSV æ–‡ä»¶\n",
    "    pd.DataFrame(video_data).to_csv(video_path, index=False)\n",
    "    pd.DataFrame(comment_data).to_csv(comment_path, index=False)\n",
    "\n",
    "    print(f\"ğŸ’¾ å·²ä¿å­˜è‡³ï¼š\\nğŸ“ è§†é¢‘æ–‡ä»¶ï¼š{video_path}\\nğŸ“ è¯„è®ºæ–‡ä»¶ï¼š{comment_path}\")\n",
    "\n",
    "# ========= å¼€å§‹é‡‡é›† ========= #\n",
    "try:\n",
    "    print(\"ğŸ” æ­£åœ¨æœç´¢è§†é¢‘...\")\n",
    "    video_ids = search_videos(SEARCH_QUERY, MAX_RESULTS)\n",
    "    print(f\"ğŸ“º å…±æ‰¾åˆ° {len(video_ids)} ä¸ªè§†é¢‘ï¼Œæ­£åœ¨è·å–è¯¦æƒ…...\")\n",
    "    video_data = get_video_details(video_ids)\n",
    "\n",
    "    print(\"ğŸ’¬ å¼€å§‹æŠ“å–æ¯ä¸ªè§†é¢‘çš„è¯„è®º...\")\n",
    "    for idx, video in enumerate(tqdm(video_data, desc=\"Fetching comments\")):\n",
    "        video_id = video['video_id']\n",
    "        comments = get_comments(video_id)\n",
    "        comment_data.extend(comments)\n",
    "\n",
    "        # å®æ—¶ä¿å­˜\n",
    "        save_data(video_data[:idx+1], comment_data)\n",
    "\n",
    "        # ä¼°ç®— API ç”¨é‡ï¼ˆä¿å®ˆä¼°ç®— 1 é¡µ â‰ˆ 1 unitï¼‰\n",
    "        units_used_this_video = max(1, len(comments) // 100 + 1)\n",
    "        estimated_units_used += units_used_this_video\n",
    "        print(f\"ğŸ“Š ä¼°ç®— API ä½¿ç”¨é‡ï¼š{estimated_units_used} / {API_UNITS_LIMIT}\")\n",
    "\n",
    "        if estimated_units_used >= SAFETY_THRESHOLD:\n",
    "            print(f\"ğŸš¨ è¾¾åˆ°å®‰å…¨é˜ˆå€¼ {SAFETY_THRESHOLD}ï¼Œè‡ªåŠ¨åœæ­¢æŠ“å–\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ æ‰‹åŠ¨ä¸­æ–­ï¼Œä¿å­˜å½“å‰æŠ“å–æ•°æ®ä¸­...\")\n",
    "    save_data(video_data, comment_data)\n",
    "    sys.exit()\n",
    "\n",
    "# ========= æ­£å¸¸å®Œæˆæ—¶æœ€ç»ˆä¿å­˜ ========= #\n",
    "save_data(video_data, comment_data, 'videos.csv', 'comments.csv')\n",
    "print(\"âœ… å…¨éƒ¨å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ videos.csv å’Œ comments.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching comments:   9%|â–‰         | 45/500 [13:40<2:18:17, 18.24s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 115\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m tqdm(video_data, desc=\u001B[33m\"\u001B[39m\u001B[33mFetching comments\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    114\u001B[39m     video_id = video[\u001B[33m'\u001B[39m\u001B[33mvideo_id\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m     comment_data.extend(\u001B[43mget_comments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo_id\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    117\u001B[39m \u001B[38;5;66;03m# ä¿å­˜æ•°æ®\u001B[39;00m\n\u001B[32m    118\u001B[39m pd.DataFrame(video_data).to_csv(\u001B[33m'\u001B[39m\u001B[33mvideos.csv\u001B[39m\u001B[33m'\u001B[39m, index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 101\u001B[39m, in \u001B[36mget_comments\u001B[39m\u001B[34m(video_id, max_comments)\u001B[39m\n\u001B[32m     98\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m next_page_token:\n\u001B[32m     99\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m     \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# æ”¾æ…¢é€Ÿåº¦ï¼Œé˜²æ­¢é™æµ\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    103\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
