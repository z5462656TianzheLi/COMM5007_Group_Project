{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T09:36:47.052477Z",
     "start_time": "2025-04-07T09:36:45.451421Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm\n",
    "\n",
    "#tests after changing to ASUS ROG\n",
    "#Works!\n",
    "\n",
    "# ========= Configuration ========= #\n",
    "SEARCH_QUERY = 'artificial intelligence'  # Search keyword\n",
    "MAX_RESULTS = 500  # Max number of videos to fetch\n",
    "API_UNITS_LIMIT = 10000  # YouTube API daily quota\n",
    "SAFETY_THRESHOLD = 9000  # Safety threshold to stop fetching\n",
    "SAVE_PATH = 'D:/COMM5007/Data/'\n",
    "\n",
    "# ========= Load API Key ========= #\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "print(\"API Key åŠ è½½æˆåŠŸï¼ˆä»…å±•ç¤ºéƒ¨åˆ†ï¼‰:\", API_KEY[:5] + \"****\")\n",
    "\n",
    "# ========= Initialize YouTube API Client ========= #\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# ========= Load Previously Saved Data ========= #\n",
    "video_csv_path = os.path.join(SAVE_PATH, 'videos.csv')\n",
    "comment_csv_path = os.path.join(SAVE_PATH, 'comments_partial.csv')\n",
    "\n",
    "if os.path.exists(video_csv_path):\n",
    "    print(\"ğŸ“ Loading previously saved video data...\")\n",
    "    video_data = pd.read_csv(video_csv_path).to_dict('records')\n",
    "else:\n",
    "    video_data = []\n",
    "\n",
    "if os.path.exists(comment_csv_path):\n",
    "    print(\"ğŸ“ Loading previously saved comment data...\")\n",
    "    comment_data = pd.read_csv(comment_csv_path).to_dict('records')\n",
    "else:\n",
    "    comment_data = []\n",
    "\n",
    "processed_video_ids = set(v['video_id'] for v in video_data)\n",
    "estimated_units_used = 0\n",
    "\n",
    "\n",
    "# ========= Search Videos ========= #\n",
    "def search_videos(query, max_results=500):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    total_fetched = 0\n",
    "\n",
    "    while total_fetched < max_results:\n",
    "        request = youtube.search().list(\n",
    "            q=query,\n",
    "            part='id',\n",
    "            type='video',\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            videos.append(item['id']['videoId'])\n",
    "        total_fetched += len(response['items'])\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    return videos\n",
    "\n",
    "\n",
    "# ========= Get Video Details ========= #\n",
    "def get_video_details(video_ids):\n",
    "    details = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part='snippet,statistics',\n",
    "            id=','.join(video_ids[i:i + 50])\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for item in response['items']:\n",
    "            details.append({\n",
    "                'video_id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'description': item['snippet']['description'],\n",
    "                'publishedAt': item['snippet']['publishedAt'],\n",
    "                'channelTitle': item['snippet']['channelTitle'],\n",
    "                'tags': ','.join(item['snippet'].get('tags', [])),\n",
    "                'categoryId': item['snippet']['categoryId'],\n",
    "                'viewCount': int(item['statistics'].get('viewCount', 0)),\n",
    "                'likeCount': int(item['statistics'].get('likeCount', 0)),\n",
    "                'commentCount': int(item['statistics'].get('commentCount', 0)),\n",
    "            })\n",
    "        time.sleep(1)\n",
    "    return details\n",
    "\n",
    "\n",
    "# ========= Get Comments ========= #\n",
    "def get_comments(video_id, max_comments=None):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat='plainText'\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comments.append({\n",
    "                    'video_id': video_id,\n",
    "                    'author': top_comment['authorDisplayName'],\n",
    "                    'comment': top_comment['textDisplay'],\n",
    "                    'likeCount': top_comment['likeCount'],\n",
    "                    'publishedAt': top_comment['publishedAt']\n",
    "                })\n",
    "\n",
    "                if max_comments and len(comments) >= max_comments:\n",
    "                    return comments\n",
    "\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "# ========= Save Data ========= #\n",
    "def save_data(video_data, comment_data, video_file='videos.csv', comment_file='comments_partial.csv'):\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "    video_path = os.path.join(SAVE_PATH, video_file)\n",
    "    comment_path = os.path.join(SAVE_PATH, comment_file)\n",
    "\n",
    "    pd.DataFrame(video_data).to_csv(video_path, index=False)\n",
    "    pd.DataFrame(comment_data).to_csv(comment_path, index=False)\n",
    "\n",
    "    print(f\"ğŸ’¾ Saved to:\\nğŸ“ Video file: {video_path}\\nğŸ“ Comment file: {comment_path}\")\n",
    "\n",
    "\n",
    "# ========= Main Execution ========= #\n",
    "try:\n",
    "    print(\"ğŸ” Searching for videos...\")\n",
    "    all_video_ids = search_videos(SEARCH_QUERY, MAX_RESULTS)\n",
    "    new_video_ids = [vid for vid in all_video_ids if vid not in processed_video_ids]\n",
    "\n",
    "    if not new_video_ids:\n",
    "        print(\"âœ… No new videos to process.\")\n",
    "        sys.exit()\n",
    "\n",
    "    print(f\"ğŸ“º Found {len(new_video_ids)} new videos to process.\")\n",
    "    video_details_today = get_video_details(new_video_ids)\n",
    "\n",
    "    # âœ… è·³è¿‡å·²æœ‰è¯„è®ºçš„è§†é¢‘ï¼Œé¿å…é‡å¤æŠ“å–\n",
    "    existing_commented_video_ids = set(c['video_id'] for c in comment_data)\n",
    "    print(f\"ğŸ§  å·²æœ‰è¯„è®ºæ•°æ®çš„è§†é¢‘æ•°é‡: {len(existing_commented_video_ids)}\")\n",
    "\n",
    "    video_details_today = [v for v in video_details_today if v['video_id'] not in existing_commented_video_ids]\n",
    "    print(f\"ğŸ“º è¿˜éœ€è¦æŠ“è¯„è®ºçš„è§†é¢‘æ•°é‡: {len(video_details_today)}\")\n",
    "\n",
    "    print(\"ğŸ’¬ Fetching comments for new videos...\")\n",
    "    for idx, video in enumerate(tqdm(video_details_today, desc=\"Fetching comments\")):\n",
    "        video_id = video['video_id']\n",
    "        comments = get_comments(video_id)\n",
    "        comment_data.extend(comments)\n",
    "        video_data.append(video)\n",
    "\n",
    "        # âœ… å®æ—¶ä¿å­˜ï¼ˆå¯é€‰ï¼‰\n",
    "        save_data(video_data, comment_data)\n",
    "\n",
    "        units_used_this_video = max(1, len(comments) // 100 + 1)\n",
    "        estimated_units_used += units_used_this_video\n",
    "        print(f\"ğŸ“Š Estimated API usage: {estimated_units_used} / {API_UNITS_LIMIT}\")\n",
    "\n",
    "        if estimated_units_used >= SAFETY_THRESHOLD:\n",
    "            print(f\"ğŸš¨ Safety threshold of {SAFETY_THRESHOLD} reached, stopping...\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ Interrupted manually, saving current data...\")\n",
    "\n",
    "# âœ… ä¿å­˜å‰å¯¹è¯„è®ºå»é‡\n",
    "seen = set()\n",
    "unique_comments = []\n",
    "for c in comment_data:\n",
    "    key = (c['video_id'], c['comment'])\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique_comments.append(c)\n",
    "comment_data = unique_comments\n",
    "\n",
    "# ========= Final Save ========= #\n",
    "save_data(video_data, comment_data, 'videos.csv', 'comments.csv')\n",
    "print(\"âœ… Completed! Data saved to videos.csv and comments.csv\")\n",
    "\n",
    "#è·å–åŸå§‹videoæ•°æ®å’Œcommentæ•°æ®åˆ°æ­¤ç»“æŸ\n",
    "######################################################################################"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key åŠ è½½æˆåŠŸï¼ˆä»…å±•ç¤ºéƒ¨åˆ†ï¼‰: AIzaS****\n",
      "ğŸ“ Loading previously saved video data...\n",
      "ğŸ“ Loading previously saved comment data...\n",
      "ğŸ” Searching for videos...\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=artificial+intelligence&part=id&type=video&maxResults=50&key=AIzaSyAPFSYKG9lnXD2pBYqLd0RUweQwMpoQfVo&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHttpError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 146\u001B[39m\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    145\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mğŸ” Searching for videos...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m     all_video_ids = \u001B[43msearch_videos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSEARCH_QUERY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMAX_RESULTS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    147\u001B[39m     new_video_ids = [vid \u001B[38;5;28;01mfor\u001B[39;00m vid \u001B[38;5;129;01min\u001B[39;00m all_video_ids \u001B[38;5;28;01mif\u001B[39;00m vid \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_video_ids]\n\u001B[32m    149\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m new_video_ids:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 57\u001B[39m, in \u001B[36msearch_videos\u001B[39m\u001B[34m(query, max_results)\u001B[39m\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m total_fetched < max_results:\n\u001B[32m     50\u001B[39m     request = youtube.search().list(\n\u001B[32m     51\u001B[39m         q=query,\n\u001B[32m     52\u001B[39m         part=\u001B[33m'\u001B[39m\u001B[33mid\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     55\u001B[39m         pageToken=next_page_token\n\u001B[32m     56\u001B[39m     )\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m     response = \u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m response[\u001B[33m'\u001B[39m\u001B[33mitems\u001B[39m\u001B[33m'\u001B[39m]:\n\u001B[32m     59\u001B[39m         videos.append(item[\u001B[33m'\u001B[39m\u001B[33mid\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mvideoId\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001B[39m, in \u001B[36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m positional_parameters_enforcement == POSITIONAL_WARNING:\n\u001B[32m    129\u001B[39m         logger.warning(message)\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\googleapiclient\\http.py:938\u001B[39m, in \u001B[36mHttpRequest.execute\u001B[39m\u001B[34m(self, http, num_retries)\u001B[39m\n\u001B[32m    936\u001B[39m     callback(resp)\n\u001B[32m    937\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m resp.status >= \u001B[32m300\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m938\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HttpError(resp, content, uri=\u001B[38;5;28mself\u001B[39m.uri)\n\u001B[32m    939\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.postproc(resp, content)\n",
      "\u001B[31mHttpError\u001B[39m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=artificial+intelligence&part=id&type=video&maxResults=50&key=AIzaSyAPFSYKG9lnXD2pBYqLd0RUweQwMpoQfVo&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
